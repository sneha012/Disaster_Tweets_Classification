{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fe361d-4154-4c37-b6a8-973db0affac7",
   "metadata": {},
   "source": [
    "## Disaster Tweets Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b92da1-771c-4a8e-a8a5-8df5e1e7fa0f",
   "metadata": {},
   "source": [
    "Write about the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3232ab-48e0-4f02-b18e-e827db494506",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2abd35-18b3-4707-8039-21a5652600b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    ShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a461796-36ea-4b73-a845-458fa1cae46b",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da045db4-31dd-4af1-882b-ef6d88ebb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unfortunately, both plans fail as the 3 are im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>crash</td>\n",
       "      <td>SLC</td>\n",
       "      <td>I hope this causes Bernie to crash and bern. S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>collide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>—pushes himself up from the chair beneath to r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Widow of CIA agent killed in 2009 Afghanistan ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>screaming</td>\n",
       "      <td>Azania</td>\n",
       "      <td>As soon as God say yes they'll be screaming we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>survived</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you've no idea the suffering and horrors that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7294</th>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oh wait, lets' not forget Anders Brevik, that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marivan, Kurdistan Province Monday, Jan 13th, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>crashed</td>\n",
       "      <td>Amphoe Mueang Nakhon Ratchasim</td>\n",
       "      <td>imagine: 15x09 airs. dean and cas share a kiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9385</th>\n",
       "      <td>snowstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494. On account of the snowstorm, all the trai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword                        location  \\\n",
       "3289          debris                             NaN   \n",
       "2672           crash                             SLC   \n",
       "2436         collide                             NaN   \n",
       "9622  suicide%20bomb                             NaN   \n",
       "8999       screaming                          Azania   \n",
       "9895        survived                             NaN   \n",
       "7294   mass%20murder                   United States   \n",
       "30            ablaze                             NaN   \n",
       "2713         crashed  Amphoe Mueang Nakhon Ratchasim   \n",
       "9385       snowstorm                             NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "3289  Unfortunately, both plans fail as the 3 are im...       0  \n",
       "2672  I hope this causes Bernie to crash and bern. S...       0  \n",
       "2436  —pushes himself up from the chair beneath to r...       0  \n",
       "9622  Widow of CIA agent killed in 2009 Afghanistan ...       1  \n",
       "8999  As soon as God say yes they'll be screaming we...       0  \n",
       "9895  you've no idea the suffering and horrors that ...       0  \n",
       "7294  Oh wait, lets' not forget Anders Brevik, that ...       1  \n",
       "30    Marivan, Kurdistan Province Monday, Jan 13th, ...       1  \n",
       "2713  imagine: 15x09 airs. dean and cas share a kiss...       0  \n",
       "9385  494. On account of the snowstorm, all the trai...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\", usecols=[\"keyword\", \"text\", \"target\", \"location\"])\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576a9623-2624-421a-87e4-4ae800f23740",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9aae7f-1dc4-4459-b572-0311d6f4c6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7395\n",
       "1    1701\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e55b9-8f9c-45a6-8702-c1f6fb4aef14",
   "metadata": {},
   "source": [
    "As we can see above, we have only 1701 examples of actual disaster tweets. To handle class imbalance, we could use a different scoring metric instead of accuracy which basically focuses on the model's performance in capturing the positive label (tweet is of a real disaster event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24dd7bb4-11e2-4421-9932-cfd3c43fcdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring metric to evaluate all the models\n",
    "\n",
    "scoring = ['precision', 'f1', 'recall', 'roc_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823948-4a1e-4eff-91f0-ebbfc98c1ebc",
   "metadata": {},
   "source": [
    "As there is a significant class imbalance, having `accuracy` as the scoring metric does not make sense, so we should use other scoring metrics to evaluate our model. For our use case, we have to minimise the `False Negatives` as we don’t want to classify an actual disaster tweet as a non disastrous tweet. So, a suitable metric can be `recall`, as a higher recall will mean that we have less number of `False Negatives`. But, we don’t want to reduce the `precision` while we increase the recall, as `True Positives` are equally important. So, a better scoring metric would be `f1`. We can also use the `auc_roc` score to be show how well the model can distinguish between the 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3758bd25-584a-4b63-bab1-a5a09a04f0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count              6370\n",
       "unique             3746\n",
       "top       United States\n",
       "freq                 80\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Location' feature\n",
    "\n",
    "train_df['location'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c2b9256-198a-425b-87bf-0c3382125123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3289                                NaN\n",
       "2672                                SLC\n",
       "2436                                NaN\n",
       "9622                                NaN\n",
       "8999                             Azania\n",
       "9895                                NaN\n",
       "7294                      United States\n",
       "30                                  NaN\n",
       "2713     Amphoe Mueang Nakhon Ratchasim\n",
       "9385                                NaN\n",
       "4355                                NaN\n",
       "8                          Accra, Ghana\n",
       "8219                     Lagos, Nigeria\n",
       "6774                                NaN\n",
       "9608                   Rohnert Park, CA\n",
       "4381                           Brighton\n",
       "1927        Hell,Hades,Mictlan,Tartarus\n",
       "8310                     Pittsburgh, PA\n",
       "391                       Mumbai, India\n",
       "3774                                NaN\n",
       "8280                      New York City\n",
       "5331                          FT. Myers\n",
       "8935                               Hell\n",
       "10621                       Quezon City\n",
       "240                       Osun, Nigeria\n",
       "6558                               آداب\n",
       "7620                       causeway bay\n",
       "8821           Scotland, United Kingdom\n",
       "8884                                NaN\n",
       "10534                         Australia\n",
       "9340      I dont know where i am. Help.\n",
       "11369                               NaN\n",
       "10379              Colorado Springs, CO\n",
       "4480                            she/her\n",
       "1201                      Aylesbury, UK\n",
       "4206                      Bath, England\n",
       "5610             St Petersburg, Florida\n",
       "7695                    Los Angeles, CA\n",
       "2648                           Thailand\n",
       "681                     Vadodara, India\n",
       "8825     Wherever socks go in the dryer\n",
       "3487                  Nola•Houston | 🇨🇴\n",
       "7627               Snohomish County, WA\n",
       "2018                                NaN\n",
       "6761                             London\n",
       "4235                            Nigeria\n",
       "4886                        Cumbria, UK\n",
       "1678                   Beacon Falls, CT\n",
       "7300                      Marching Home\n",
       "5628                                  🔞\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['location'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fc079-e2b1-4e4f-bf7c-9136ea302123",
   "metadata": {},
   "source": [
    "As we can see above, there is a challenge in using the `location` column. The feature is quite messy. There are missing values, emoticons (flags), different languages, unrelated information (she/her), and free text comment (e.g., \"I dont know where i am. Help.\" Here are a few reasons why it would not be a good decision to use the `location` feature for our model training :\n",
    "\n",
    "1) The `location` column has a many null values (NaN) which will have to be handled.\n",
    "2) Most `location` values are not in an appropriate format (includes special characters and emojis)\n",
    "3) It has Countries and cities mixed with each other and there is no standardization. \n",
    "4) Few values are not even location values and cannot be used.\n",
    "5) There are 3747 unique values of location values and it would be very expensive and inefficient to apply transformations like One hot Encoding on this column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd9b90-e3fd-4a55-950d-8ba314d020a1",
   "metadata": {},
   "source": [
    "### Identifying features and building Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ba82e3-a43b-4db3-9ead-7d0d7e8c8ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9096 entries, 3289 to 7336\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   keyword   9096 non-null   object\n",
      " 1   location  6370 non-null   object\n",
      " 2   text      9096 non-null   object\n",
      " 3   target    9096 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 355.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389fcaba-2a0f-4178-8677-6c8d48bf0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunderstorm    74\n",
      "flattened       74\n",
      "sirens          73\n",
      "drown           71\n",
      "stretcher       71\n",
      "                ..\n",
      "blown%20up      11\n",
      "siren           10\n",
      "rainstorm       10\n",
      "deluged          7\n",
      "tsunami          6\n",
      "Name: keyword, Length: 219, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"keyword\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0c9da7-5211-48c7-ba2f-e5a9f6c77f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column transformer\n",
    "categorical_features = [\"keyword\"]\n",
    "drop_features = [\"location\"]\n",
    "text_feature = \"text\"\n",
    "target = \"target\"\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (CountVectorizer(stop_words=\"english\", lowercase=False), text_feature),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523194d-3e1c-4681-81b0-ce35a5fcd8a8",
   "metadata": {},
   "source": [
    "We are dropping the `location` feature due to the NULL values, non-standardized values and reliability of the values in the column. We apply the Count Vectorizer transformation on the `text` column in order to convert the text column to numeric vectors which the model can understand. We apply One Hot Encoding transformation on the `keyword` column so the model can also consider the prominent disaster related keywords as part of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72838b2-6e63-4d49-855c-836bfc8ba390",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08fde8b-6868-457b-934b-64531c111a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557bbe34-d961-4488-9231-6b7a23c37a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to report mean cross validation scores for different models\n",
    "\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08c183-f100-42d9-a29f-dd08ae4a30e0",
   "metadata": {},
   "source": [
    "#### Dummy Classifier (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f88130-1da6-4e83-a2b5-accd9f77e37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.189 (+/- 0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.190 (+/- 0.010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.190 (+/- 0.019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.188 (+/- 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.192 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.187 (+/- 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.501 (+/- 0.015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_roc_auc</th>\n",
       "      <td>0.496 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dummy\n",
       "fit_time         0.001 (+/- 0.001)\n",
       "score_time       0.004 (+/- 0.002)\n",
       "test_precision   0.189 (+/- 0.022)\n",
       "train_precision  0.190 (+/- 0.010)\n",
       "test_f1          0.190 (+/- 0.019)\n",
       "train_f1         0.188 (+/- 0.009)\n",
       "test_recall      0.192 (+/- 0.018)\n",
       "train_recall     0.187 (+/- 0.009)\n",
       "test_roc_auc     0.501 (+/- 0.015)\n",
       "train_roc_auc    0.496 (+/- 0.005)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc08d2-3984-46a2-8d1b-3174041a5f6d",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab7c895-7404-488f-88d8-3fe1b16241b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>logistic regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.001)</td>\n",
       "      <td>0.297 (+/- 0.046)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.002)</td>\n",
       "      <td>0.047 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.189 (+/- 0.022)</td>\n",
       "      <td>0.804 (+/- 0.017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.190 (+/- 0.010)</td>\n",
       "      <td>0.997 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.190 (+/- 0.019)</td>\n",
       "      <td>0.616 (+/- 0.027)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.188 (+/- 0.009)</td>\n",
       "      <td>0.971 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.192 (+/- 0.018)</td>\n",
       "      <td>0.499 (+/- 0.030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.187 (+/- 0.009)</td>\n",
       "      <td>0.946 (+/- 0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.501 (+/- 0.015)</td>\n",
       "      <td>0.890 (+/- 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_roc_auc</th>\n",
       "      <td>0.496 (+/- 0.005)</td>\n",
       "      <td>0.999 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dummy logistic regression\n",
       "fit_time         0.001 (+/- 0.001)   0.297 (+/- 0.046)\n",
       "score_time       0.004 (+/- 0.002)   0.047 (+/- 0.001)\n",
       "test_precision   0.189 (+/- 0.022)   0.804 (+/- 0.017)\n",
       "train_precision  0.190 (+/- 0.010)   0.997 (+/- 0.001)\n",
       "test_f1          0.190 (+/- 0.019)   0.616 (+/- 0.027)\n",
       "train_f1         0.188 (+/- 0.009)   0.971 (+/- 0.002)\n",
       "test_recall      0.192 (+/- 0.018)   0.499 (+/- 0.030)\n",
       "train_recall     0.187 (+/- 0.009)   0.946 (+/- 0.003)\n",
       "test_roc_auc     0.501 (+/- 0.015)   0.890 (+/- 0.012)\n",
       "train_roc_auc    0.496 (+/- 0.005)   0.999 (+/- 0.000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=2000))\n",
    "results[\"logistic regression\"] = mean_std_cross_val_scores(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c93dac-7adf-4026-9041-0fb4df4a18b2",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cda26540-e952-4c00-99f6-ffb04645550a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28010"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the Vocabulary of the count vectorizer\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "vocab = (\n",
    "    pipe_lr.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"countvectorizer\"]\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1cddc47-0dab-4aa2-ac54-354e0f2957e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "param_dist = {\n",
    "    \"columntransformer__countvectorizer__max_features\": randint(5_000, len(vocab)),\n",
    "    \"logisticregression__C\": loguniform(1e-3, 1e3),\n",
    "    \"logisticregression__class_weight\": [\"balanced\", None],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_lr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"f1\",\n",
    "    random_state=123,\n",
    ")\n",
    "random_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d213aa4-bda0-4378-b394-8f9086456c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer__countvectorizer__max_features': 26323,\n",
       " 'logisticregression__C': 0.38474818517522286,\n",
       " 'logisticregression__class_weight': 'balanced'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbb09701-d3e6-49ab-af20-f821c9730849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6646678202469128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "464f5e1b-b90c-4a97-a301-a6d18fddc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_feats = random_search.best_params_[\n",
    "    \"columntransformer__countvectorizer__max_features\"\n",
    "]\n",
    "best_c = random_search.best_params_[\"logisticregression__C\"]\n",
    "best_class_weight = random_search.best_params_[\"logisticregression__class_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1869b2e9-d64e-484c-b6ff-0610935a625a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>logistic regression</th>\n",
       "      <th>logistic regression (optimized params)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.001)</td>\n",
       "      <td>0.297 (+/- 0.046)</td>\n",
       "      <td>0.273 (+/- 0.073)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.002)</td>\n",
       "      <td>0.047 (+/- 0.001)</td>\n",
       "      <td>0.050 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.189 (+/- 0.022)</td>\n",
       "      <td>0.804 (+/- 0.017)</td>\n",
       "      <td>0.670 (+/- 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.190 (+/- 0.010)</td>\n",
       "      <td>0.997 (+/- 0.001)</td>\n",
       "      <td>0.906 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.190 (+/- 0.019)</td>\n",
       "      <td>0.616 (+/- 0.027)</td>\n",
       "      <td>0.665 (+/- 0.017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.188 (+/- 0.009)</td>\n",
       "      <td>0.971 (+/- 0.002)</td>\n",
       "      <td>0.946 (+/- 0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.192 (+/- 0.018)</td>\n",
       "      <td>0.499 (+/- 0.030)</td>\n",
       "      <td>0.660 (+/- 0.030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.187 (+/- 0.009)</td>\n",
       "      <td>0.946 (+/- 0.003)</td>\n",
       "      <td>0.990 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.501 (+/- 0.015)</td>\n",
       "      <td>0.890 (+/- 0.012)</td>\n",
       "      <td>0.891 (+/- 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_roc_auc</th>\n",
       "      <td>0.496 (+/- 0.005)</td>\n",
       "      <td>0.999 (+/- 0.000)</td>\n",
       "      <td>0.998 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dummy logistic regression  \\\n",
       "fit_time         0.001 (+/- 0.001)   0.297 (+/- 0.046)   \n",
       "score_time       0.004 (+/- 0.002)   0.047 (+/- 0.001)   \n",
       "test_precision   0.189 (+/- 0.022)   0.804 (+/- 0.017)   \n",
       "train_precision  0.190 (+/- 0.010)   0.997 (+/- 0.001)   \n",
       "test_f1          0.190 (+/- 0.019)   0.616 (+/- 0.027)   \n",
       "train_f1         0.188 (+/- 0.009)   0.971 (+/- 0.002)   \n",
       "test_recall      0.192 (+/- 0.018)   0.499 (+/- 0.030)   \n",
       "train_recall     0.187 (+/- 0.009)   0.946 (+/- 0.003)   \n",
       "test_roc_auc     0.501 (+/- 0.015)   0.890 (+/- 0.012)   \n",
       "train_roc_auc    0.496 (+/- 0.005)   0.999 (+/- 0.000)   \n",
       "\n",
       "                logistic regression (optimized params)  \n",
       "fit_time                             0.273 (+/- 0.073)  \n",
       "score_time                           0.050 (+/- 0.004)  \n",
       "test_precision                       0.670 (+/- 0.013)  \n",
       "train_precision                      0.906 (+/- 0.004)  \n",
       "test_f1                              0.665 (+/- 0.017)  \n",
       "train_f1                             0.946 (+/- 0.003)  \n",
       "test_recall                          0.660 (+/- 0.030)  \n",
       "train_recall                         0.990 (+/- 0.002)  \n",
       "test_roc_auc                         0.891 (+/- 0.012)  \n",
       "train_roc_auc                        0.998 (+/- 0.000)  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"logistic regression (optimized params)\"] = mean_std_cross_val_scores(\n",
    "    random_search.best_estimator_,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    return_train_score=True,\n",
    "    scoring=scoring,\n",
    ")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b44079-49ce-4011-9bba-889ef936ee2f",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f64eec79-fcaa-486a-8840-38ce25801744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/snehajhaveri/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/snehajhaveri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "968926a5-b99b-4200-b6c8-ca6854d004ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features relevant to the data and the problem \n",
    "\n",
    "def get_relative_length(text, TWITTER_ALLOWED_CHARS=280.0):\n",
    "    \"\"\"\n",
    "    Returns the relative length of text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------\n",
    "    TWITTER_ALLOWED_CHARS: (float)\n",
    "    the denominator for finding relative length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    relative length of text: (float)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(text) / TWITTER_ALLOWED_CHARS\n",
    "\n",
    "def get_length_in_words(text):\n",
    "    \"\"\"\n",
    "    Returns the length of the text in words.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    length of tokenized text: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns the compound score representing the sentiment: -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    The compound score is a normalized score calculated by summing the valence scores of each word in the lexicon.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    sentiment of the text: (str)\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "883a08cd-1630-4067-8602-3d84fc3b5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(n_words=train_df[\"text\"].apply(get_length_in_words))\n",
    "train_df = train_df.assign(vader_sentiment=train_df[\"text\"].apply(get_sentiment))\n",
    "train_df = train_df.assign(rel_char_len=train_df[\"text\"].apply(get_relative_length))\n",
    "\n",
    "test_df = test_df.assign(n_words=test_df[\"text\"].apply(get_length_in_words))\n",
    "test_df = test_df.assign(vader_sentiment=test_df[\"text\"].apply(get_sentiment))\n",
    "test_df = test_df.assign(rel_char_len=test_df[\"text\"].apply(get_relative_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81e78841-a3bd-48fd-9de0-1fb0b443499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacymoji import Emoji\n",
    "import en_core_web_md  # pre-trained model\n",
    "import spacy\n",
    "\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38c7981c-59e1-4b7d-a08f-b1f9ea496d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"emoji\", first=True);\n",
    "def get_emojis(text):\n",
    "    \"\"\"\n",
    "    Returns the number of emojis in the given text\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    count of emojis in the text : int\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return len(doc._.emoji)\n",
    "train_df['num_emojis']=train_df[\"text\"].apply(get_emojis)\n",
    "test_df['num_emojis']=test_df[\"text\"].apply(get_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bddbb-f048-4022-a837-c58ee9c74d90",
   "metadata": {},
   "source": [
    "**Reasoning**\n",
    "\n",
    "As we can see in our dataset, most formal tweets related to disaster events have very few emojis. In contrast, fake reviews and fake tweets tend to have comparatively more emojis. So, we are creating a new feature which counts the number of emojis per tweet and this if the number of emojis is higher then the chance it being a disaster tweet is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "632ec128-f333-47a4-8023-d543e6abba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new feature for Mention Count\n",
    "\n",
    "train_df['mention_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "test_df['mention_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675a50d-d884-4132-82bc-4d5aeeba4fec",
   "metadata": {},
   "source": [
    "**Reasoning**\n",
    "\n",
    "Usually formal tweets about disasters consists of mentions which starts with '@'. Most formal disaster posts mentions important authorities of the city. So, there is a chance that the tweets that consists of '@' are authentic disaster related tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14b930a2-10dc-48c2-b21d-787e40e50ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new feature for Punctuation Count \n",
    "\n",
    "train_df['punctuation_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test_df['punctuation_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63623b-bd8a-4c15-9ffd-4460bdc80046",
   "metadata": {},
   "source": [
    "**Reasoning**\n",
    "\n",
    "Formal tweets about disasters are usually written with proper punctuations compared to fake tweets and movie reviews. Non-disaster tweets could have more typos and be missing punctuations than disaster tweets because they are coming from individual users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fff40b2b-95e0-4cdf-a8d8-3dfc04570c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>n_words</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>rel_char_len</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>punctuation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unfortunately, both plans fail as the 3 are im...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.7650</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>crash</td>\n",
       "      <td>SLC</td>\n",
       "      <td>I hope this causes Bernie to crash and bern. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.5697</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>collide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>—pushes himself up from the chair beneath to r...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.439286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Widow of CIA agent killed in 2009 Afghanistan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.9460</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>screaming</td>\n",
       "      <td>Azania</td>\n",
       "      <td>As soon as God say yes they'll be screaming we...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.203571</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword location  \\\n",
       "3289          debris      NaN   \n",
       "2672           crash      SLC   \n",
       "2436         collide      NaN   \n",
       "9622  suicide%20bomb      NaN   \n",
       "8999       screaming   Azania   \n",
       "\n",
       "                                                   text  target  n_words  \\\n",
       "3289  Unfortunately, both plans fail as the 3 are im...       0       22   \n",
       "2672  I hope this causes Bernie to crash and bern. S...       0       18   \n",
       "2436  —pushes himself up from the chair beneath to r...       0       21   \n",
       "9622  Widow of CIA agent killed in 2009 Afghanistan ...       1       20   \n",
       "8999  As soon as God say yes they'll be screaming we...       0       14   \n",
       "\n",
       "      vader_sentiment  rel_char_len  mention_count  num_emojis  \\\n",
       "3289          -0.7650      0.425000              0           0   \n",
       "2672          -0.5697      0.267857              0           0   \n",
       "2436           0.0000      0.439286              0           0   \n",
       "9622          -0.9460      0.428571              0           0   \n",
       "8999           0.2960      0.203571              0           2   \n",
       "\n",
       "      punctuation_count  \n",
       "3289                  2  \n",
       "2672                  7  \n",
       "2436                  6  \n",
       "9622                  5  \n",
       "8999                  1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fb63a-0812-4d4a-8536-4dccc7f76139",
   "metadata": {},
   "source": [
    "### Building pipeline with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5bfee64-4bfb-40ce-a3f9-e656af91b2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['keyword', 'location', 'text', 'target', 'n_words', 'vader_sentiment',\n",
       "       'rel_char_len', 'mention_count', 'num_emojis', 'punctuation_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05352d85-7d9c-45b1-84bc-47409b7619cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95b3eff3-9791-47b4-81d5-0c5e3fb4e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating the features based on the transformations\n",
    "\n",
    "drop_features = ['location']\n",
    "keyword_features = \"keyword\"\n",
    "text_features = \"text\"\n",
    "numeric_features = ['n_words', 'vader_sentiment', 'rel_char_len', 'num_emojis', 'mention_count', 'punctuation_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ff93c3b-71a6-4886-a9ee-b1b3476fa526",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), categorical_features),\n",
    "    (CountVectorizer(stop_words=\"english\", max_features=best_max_feats), text_feature),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac951a5a-98a4-4984-811e-4354fe206db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>logistic regression</th>\n",
       "      <th>logistic regression (optimized params)</th>\n",
       "      <th>logistic regression (new features)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.001)</td>\n",
       "      <td>0.297 (+/- 0.046)</td>\n",
       "      <td>0.273 (+/- 0.073)</td>\n",
       "      <td>0.375 (+/- 0.045)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.002)</td>\n",
       "      <td>0.047 (+/- 0.001)</td>\n",
       "      <td>0.050 (+/- 0.004)</td>\n",
       "      <td>0.060 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.189 (+/- 0.022)</td>\n",
       "      <td>0.804 (+/- 0.017)</td>\n",
       "      <td>0.670 (+/- 0.013)</td>\n",
       "      <td>0.657 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.190 (+/- 0.010)</td>\n",
       "      <td>0.997 (+/- 0.001)</td>\n",
       "      <td>0.906 (+/- 0.004)</td>\n",
       "      <td>0.880 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.190 (+/- 0.019)</td>\n",
       "      <td>0.616 (+/- 0.027)</td>\n",
       "      <td>0.665 (+/- 0.017)</td>\n",
       "      <td>0.678 (+/- 0.016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.188 (+/- 0.009)</td>\n",
       "      <td>0.971 (+/- 0.002)</td>\n",
       "      <td>0.946 (+/- 0.003)</td>\n",
       "      <td>0.928 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.192 (+/- 0.018)</td>\n",
       "      <td>0.499 (+/- 0.030)</td>\n",
       "      <td>0.660 (+/- 0.030)</td>\n",
       "      <td>0.701 (+/- 0.036)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.187 (+/- 0.009)</td>\n",
       "      <td>0.946 (+/- 0.003)</td>\n",
       "      <td>0.990 (+/- 0.002)</td>\n",
       "      <td>0.982 (+/- 0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.501 (+/- 0.015)</td>\n",
       "      <td>0.890 (+/- 0.012)</td>\n",
       "      <td>0.891 (+/- 0.012)</td>\n",
       "      <td>0.898 (+/- 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_roc_auc</th>\n",
       "      <td>0.496 (+/- 0.005)</td>\n",
       "      <td>0.999 (+/- 0.000)</td>\n",
       "      <td>0.998 (+/- 0.000)</td>\n",
       "      <td>0.996 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dummy logistic regression  \\\n",
       "fit_time         0.001 (+/- 0.001)   0.297 (+/- 0.046)   \n",
       "score_time       0.004 (+/- 0.002)   0.047 (+/- 0.001)   \n",
       "test_precision   0.189 (+/- 0.022)   0.804 (+/- 0.017)   \n",
       "train_precision  0.190 (+/- 0.010)   0.997 (+/- 0.001)   \n",
       "test_f1          0.190 (+/- 0.019)   0.616 (+/- 0.027)   \n",
       "train_f1         0.188 (+/- 0.009)   0.971 (+/- 0.002)   \n",
       "test_recall      0.192 (+/- 0.018)   0.499 (+/- 0.030)   \n",
       "train_recall     0.187 (+/- 0.009)   0.946 (+/- 0.003)   \n",
       "test_roc_auc     0.501 (+/- 0.015)   0.890 (+/- 0.012)   \n",
       "train_roc_auc    0.496 (+/- 0.005)   0.999 (+/- 0.000)   \n",
       "\n",
       "                logistic regression (optimized params)  \\\n",
       "fit_time                             0.273 (+/- 0.073)   \n",
       "score_time                           0.050 (+/- 0.004)   \n",
       "test_precision                       0.670 (+/- 0.013)   \n",
       "train_precision                      0.906 (+/- 0.004)   \n",
       "test_f1                              0.665 (+/- 0.017)   \n",
       "train_f1                             0.946 (+/- 0.003)   \n",
       "test_recall                          0.660 (+/- 0.030)   \n",
       "train_recall                         0.990 (+/- 0.002)   \n",
       "test_roc_auc                         0.891 (+/- 0.012)   \n",
       "train_roc_auc                        0.998 (+/- 0.000)   \n",
       "\n",
       "                logistic regression (new features)  \n",
       "fit_time                         0.375 (+/- 0.045)  \n",
       "score_time                       0.060 (+/- 0.001)  \n",
       "test_precision                   0.657 (+/- 0.007)  \n",
       "train_precision                  0.880 (+/- 0.007)  \n",
       "test_f1                          0.678 (+/- 0.016)  \n",
       "train_f1                         0.928 (+/- 0.005)  \n",
       "test_recall                      0.701 (+/- 0.036)  \n",
       "train_recall                     0.982 (+/- 0.003)  \n",
       "test_roc_auc                     0.898 (+/- 0.012)  \n",
       "train_roc_auc                    0.996 (+/- 0.000)  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with new features\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(max_iter=1000, class_weight=best_class_weight, C=best_c),\n",
    ")\n",
    "\n",
    "results[\"logistic regression (new features)\"] = mean_std_cross_val_scores(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714f8c4-ddde-43fc-b00b-c302f599a24b",
   "metadata": {},
   "source": [
    "On adding new features, we can notice that the recall has improved and so has the f1 score compared to our model with original features. However, the precision has reduced slightly on the addition of new features compared to the score we achieved with original features. The roc_auc score also seems to have become slightly better after adding the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7093027-4863-4ca0-aeba-68fff040376e",
   "metadata": {},
   "source": [
    "### Analyzing Feature Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8a17f02-9b11-4307-924a-2a474bddadb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/envs/573/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thunderstorm</th>\n",
       "      <td>1.603396</td>\n",
       "      <td>1.603396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0_windstorm</th>\n",
       "      <td>1.452117</td>\n",
       "      <td>1.452117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>1.449866</td>\n",
       "      <td>1.449866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>1.440630</td>\n",
       "      <td>1.440630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescued</th>\n",
       "      <td>1.439215</td>\n",
       "      <td>1.439215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collision</th>\n",
       "      <td>1.372419</td>\n",
       "      <td>1.372419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ukrainian</th>\n",
       "      <td>1.327828</td>\n",
       "      <td>1.327828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road</th>\n",
       "      <td>1.309697</td>\n",
       "      <td>1.309697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0_buildings%20on%20fire</th>\n",
       "      <td>1.255956</td>\n",
       "      <td>1.255956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hitchin</th>\n",
       "      <td>1.169800</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carried</th>\n",
       "      <td>1.168212</td>\n",
       "      <td>1.168212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massive</th>\n",
       "      <td>1.157179</td>\n",
       "      <td>1.157179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavy</th>\n",
       "      <td>1.133360</td>\n",
       "      <td>1.133360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windstorm</th>\n",
       "      <td>1.124327</td>\n",
       "      <td>1.124327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kashmir</th>\n",
       "      <td>1.120743</td>\n",
       "      <td>1.120743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0_derailment</th>\n",
       "      <td>1.117104</td>\n",
       "      <td>1.117104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innocent</th>\n",
       "      <td>1.108606</td>\n",
       "      <td>1.108606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0_thunderstorm</th>\n",
       "      <td>1.099692</td>\n",
       "      <td>1.099692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit</th>\n",
       "      <td>1.098070</td>\n",
       "      <td>1.098070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injured</th>\n",
       "      <td>1.095158</td>\n",
       "      <td>1.095158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          coefficient  magnitude\n",
       "thunderstorm                 1.603396   1.603396\n",
       "x0_windstorm                 1.452117   1.452117\n",
       "died                         1.449866   1.449866\n",
       "survived                     1.440630   1.440630\n",
       "rescued                      1.439215   1.439215\n",
       "collision                    1.372419   1.372419\n",
       "ukrainian                    1.327828   1.327828\n",
       "road                         1.309697   1.309697\n",
       "x0_buildings%20on%20fire     1.255956   1.255956\n",
       "hitchin                      1.169800   1.169800\n",
       "carried                      1.168212   1.168212\n",
       "massive                      1.157179   1.157179\n",
       "heavy                        1.133360   1.133360\n",
       "windstorm                    1.124327   1.124327\n",
       "kashmir                      1.120743   1.120743\n",
       "x0_derailment                1.117104   1.117104\n",
       "innocent                     1.108606   1.108606\n",
       "x0_thunderstorm              1.099692   1.099692\n",
       "hit                          1.098070   1.098070\n",
       "injured                      1.095158   1.095158"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "feature_names = (\n",
    "    numeric_features\n",
    "    + list(\n",
    "        pipe_lr.named_steps[\"columntransformer\"]\n",
    "        .named_transformers_[\"onehotencoder\"]\n",
    "        .get_feature_names()\n",
    "    )\n",
    "    + list(\n",
    "        pipe_lr.named_steps[\"columntransformer\"]\n",
    "        .named_transformers_[\"countvectorizer\"]\n",
    "        .get_feature_names_out()\n",
    "    )\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"coefficient\": pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist(),\n",
    "    \"magnitude\": np.absolute(\n",
    "        pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist()\n",
    "    ),\n",
    "}\n",
    "coef_df = pd.DataFrame(data, index=feature_names).sort_values(\n",
    "    \"magnitude\", ascending=False\n",
    ")\n",
    "coef_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935231e5-1c4f-48b1-9f34-3821ce6ab2ec",
   "metadata": {},
   "source": [
    "The top features shown above makes sense. As we can see above, features like `windstorm`, `rescued`, `thunderstorm`, `died`, `survived`, `carried` etc seems to be important features in the context of real tweets related to disastrous events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3dd82c6-0bc1-463d-8727-71133e9cbcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rel_char_len</th>\n",
       "      <td>0.645789</td>\n",
       "      <td>0.645789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_count</th>\n",
       "      <td>-0.043281</td>\n",
       "      <td>0.043281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mention_count</th>\n",
       "      <td>-0.048048</td>\n",
       "      <td>0.048048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_emojis</th>\n",
       "      <td>-0.083662</td>\n",
       "      <td>0.083662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_sentiment</th>\n",
       "      <td>-0.406554</td>\n",
       "      <td>0.406554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_words</th>\n",
       "      <td>-0.667181</td>\n",
       "      <td>0.667181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   coefficient  magnitude\n",
       "rel_char_len          0.645789   0.645789\n",
       "punctuation_count    -0.043281   0.043281\n",
       "mention_count        -0.048048   0.048048\n",
       "num_emojis           -0.083662   0.083662\n",
       "vader_sentiment      -0.406554   0.406554\n",
       "n_words              -0.667181   0.667181"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the importance of newly added features\n",
    "\n",
    "extracted_feats = ['n_words', 'vader_sentiment', 'rel_char_len', 'num_emojis', 'mention_count', 'punctuation_count']\n",
    "\n",
    "coef_df.loc[extracted_feats].sort_values(\"coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee5cf8-e369-4eb0-87c7-485ad60ee3f7",
   "metadata": {},
   "source": [
    "Some coefficients do make sense -\n",
    "\n",
    "1) Presence of more emojis seem to drive predictions in the non-disaster direction.\n",
    "2) The coefficient of vader_sentiment feature is negative, suggesting that bigger sentiment score (i.e., positive sentiment) is pushing us towards non-disaster tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a512a61-32b7-4264-939d-3c508857cd29",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1aa163b-bdf4-4fe1-a6e7-32e443533e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score\n",
      "0.7262313860252003\n",
      "Precision Score\n",
      "0.6891304347826087\n",
      "Recall Score\n",
      "0.7675544794188862\n",
      "ROC AUC Score\n",
      "0.8453570355181481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "print('F1 Score')\n",
    "print(f1_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('Precision Score')\n",
    "print(precision_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('Recall Score')\n",
    "print(recall_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('ROC AUC Score')\n",
    "print(roc_auc_score(y_test, pipe_lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adff5b3-e51f-40ec-a289-10f141df7fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
