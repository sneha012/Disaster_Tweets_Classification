{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fe361d-4154-4c37-b6a8-973db0affac7",
   "metadata": {},
   "source": [
    "## Disaster Tweets Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b92da1-771c-4a8e-a8a5-8df5e1e7fa0f",
   "metadata": {},
   "source": [
    "Write about the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3232ab-48e0-4f02-b18e-e827db494506",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2abd35-18b3-4707-8039-21a5652600b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    ShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a461796-36ea-4b73-a845-458fa1cae46b",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da045db4-31dd-4af1-882b-ef6d88ebb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unfortunately, both plans fail as the 3 are im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>crash</td>\n",
       "      <td>SLC</td>\n",
       "      <td>I hope this causes Bernie to crash and bern. S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>collide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>â€”pushes himself up from the chair beneath to r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Widow of CIA agent killed in 2009 Afghanistan ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>screaming</td>\n",
       "      <td>Azania</td>\n",
       "      <td>As soon as God say yes they'll be screaming we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>survived</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you've no idea the suffering and horrors that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7294</th>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oh wait, lets' not forget Anders Brevik, that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marivan, Kurdistan Province Monday, Jan 13th, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>crashed</td>\n",
       "      <td>Amphoe Mueang Nakhon Ratchasim</td>\n",
       "      <td>imagine: 15x09 airs. dean and cas share a kiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9385</th>\n",
       "      <td>snowstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494. On account of the snowstorm, all the trai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword                        location  \\\n",
       "3289          debris                             NaN   \n",
       "2672           crash                             SLC   \n",
       "2436         collide                             NaN   \n",
       "9622  suicide%20bomb                             NaN   \n",
       "8999       screaming                          Azania   \n",
       "9895        survived                             NaN   \n",
       "7294   mass%20murder                   United States   \n",
       "30            ablaze                             NaN   \n",
       "2713         crashed  Amphoe Mueang Nakhon Ratchasim   \n",
       "9385       snowstorm                             NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "3289  Unfortunately, both plans fail as the 3 are im...       0  \n",
       "2672  I hope this causes Bernie to crash and bern. S...       0  \n",
       "2436  â€”pushes himself up from the chair beneath to r...       0  \n",
       "9622  Widow of CIA agent killed in 2009 Afghanistan ...       1  \n",
       "8999  As soon as God say yes they'll be screaming we...       0  \n",
       "9895  you've no idea the suffering and horrors that ...       0  \n",
       "7294  Oh wait, lets' not forget Anders Brevik, that ...       1  \n",
       "30    Marivan, Kurdistan Province Monday, Jan 13th, ...       1  \n",
       "2713  imagine: 15x09 airs. dean and cas share a kiss...       0  \n",
       "9385  494. On account of the snowstorm, all the trai...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\", usecols=[\"keyword\", \"text\", \"target\", \"location\"])\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576a9623-2624-421a-87e4-4ae800f23740",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9aae7f-1dc4-4459-b572-0311d6f4c6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7395\n",
       "1    1701\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e55b9-8f9c-45a6-8702-c1f6fb4aef14",
   "metadata": {},
   "source": [
    "As we can see above, we have only 1701 examples of actual disaster tweets. To handle class imbalance, we could use a different scoring metric instead of accuracy which basically focuses on the model's performance in capturing the positive label (tweet is of a real disaster event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24dd7bb4-11e2-4421-9932-cfd3c43fcdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring metric to evaluate all the models\n",
    "\n",
    "scoring = ['precision', 'f1', 'recall', 'roc_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823948-4a1e-4eff-91f0-ebbfc98c1ebc",
   "metadata": {},
   "source": [
    "As there is a significant class imbalance, having `accuracy` as the scoring metric does not make sense, so we should use other scoring metrics to evaluate our model. For our use case, we have to minimise the `False Negatives` as we donâ€™t want to classify an actual disaster tweet as a non disastrous tweet. So, a suitable metric can be `recall`, as a higher recall will mean that we have less number of `False Negatives`. But, we donâ€™t want to reduce the `precision` while we increase the recall, as `True Positives` are equally important. So, a better scoring metric would be `f1`. We can also use the `auc_roc` score to be show how well the model can distinguish between the 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3758bd25-584a-4b63-bab1-a5a09a04f0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count              6370\n",
       "unique             3746\n",
       "top       United States\n",
       "freq                 80\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Location' feature\n",
    "\n",
    "train_df['location'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c2b9256-198a-425b-87bf-0c3382125123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3289                                NaN\n",
       "2672                                SLC\n",
       "2436                                NaN\n",
       "9622                                NaN\n",
       "8999                             Azania\n",
       "9895                                NaN\n",
       "7294                      United States\n",
       "30                                  NaN\n",
       "2713     Amphoe Mueang Nakhon Ratchasim\n",
       "9385                                NaN\n",
       "4355                                NaN\n",
       "8                          Accra, Ghana\n",
       "8219                     Lagos, Nigeria\n",
       "6774                                NaN\n",
       "9608                   Rohnert Park, CA\n",
       "4381                           Brighton\n",
       "1927        Hell,Hades,Mictlan,Tartarus\n",
       "8310                     Pittsburgh, PA\n",
       "391                       Mumbai, India\n",
       "3774                                NaN\n",
       "8280                      New York City\n",
       "5331                          FT. Myers\n",
       "8935                               Hell\n",
       "10621                       Quezon City\n",
       "240                       Osun, Nigeria\n",
       "6558                               Ø¢Ø¯Ø§Ø¨\n",
       "7620                       causeway bay\n",
       "8821           Scotland, United Kingdom\n",
       "8884                                NaN\n",
       "10534                         Australia\n",
       "9340      I dont know where i am. Help.\n",
       "11369                               NaN\n",
       "10379              Colorado Springs, CO\n",
       "4480                            she/her\n",
       "1201                      Aylesbury, UK\n",
       "4206                      Bath, England\n",
       "5610             St Petersburg, Florida\n",
       "7695                    Los Angeles, CA\n",
       "2648                           Thailand\n",
       "681                     Vadodara, India\n",
       "8825     Wherever socks go in the dryer\n",
       "3487                  Nolaâ€¢Houston | ðŸ‡¨ðŸ‡´\n",
       "7627               Snohomish County, WA\n",
       "2018                                NaN\n",
       "6761                             London\n",
       "4235                            Nigeria\n",
       "4886                        Cumbria, UK\n",
       "1678                   Beacon Falls, CT\n",
       "7300                      Marching Home\n",
       "5628                                  ðŸ”ž\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['location'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fc079-e2b1-4e4f-bf7c-9136ea302123",
   "metadata": {},
   "source": [
    "As we can see above, there is a challenge in using the `location` column. The feature is quite messy. There are missing values, emoticons (flags), different languages, unrelated information (she/her), and free text comment (e.g., \"I dont know where i am. Help.\" Here are a few reasons why it would not be a good decision to use the `location` feature for our model training :\n",
    "\n",
    "1) The `location` column has a many null values (NaN) which will have to be handled.\n",
    "2) Most `location` values are not in an appropriate format (includes special characters and emojis)\n",
    "3) It has Countries and cities mixed with each other and there is no standardization. \n",
    "4) Few values are not even location values and cannot be used.\n",
    "5) There are 3747 unique values of location values and it would be very expensive and inefficient to apply transformations like One hot Encoding on this column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd9b90-e3fd-4a55-950d-8ba314d020a1",
   "metadata": {},
   "source": [
    "### Identifying features and building Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ba82e3-a43b-4db3-9ead-7d0d7e8c8ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9096 entries, 3289 to 7336\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   keyword   9096 non-null   object\n",
      " 1   location  6370 non-null   object\n",
      " 2   text      9096 non-null   object\n",
      " 3   target    9096 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 355.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389fcaba-2a0f-4178-8677-6c8d48bf0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunderstorm    74\n",
      "flattened       74\n",
      "sirens          73\n",
      "drown           71\n",
      "stretcher       71\n",
      "                ..\n",
      "blown%20up      11\n",
      "siren           10\n",
      "rainstorm       10\n",
      "deluged          7\n",
      "tsunami          6\n",
      "Name: keyword, Length: 219, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"keyword\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0c9da7-5211-48c7-ba2f-e5a9f6c77f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column transformer\n",
    "categorical_features = [\"keyword\"]\n",
    "drop_features = [\"location\"]\n",
    "text_feature = \"text\"\n",
    "target = \"target\"\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (CountVectorizer(stop_words=\"english\", lowercase=False), text_feature),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523194d-3e1c-4681-81b0-ce35a5fcd8a8",
   "metadata": {},
   "source": [
    "We are dropping the `location` feature due to the NULL values, non-standardized values and reliability of the values in the column. We apply the Count Vectorizer transformation on the `text` column in order to convert the text column to numeric vectors which the model can understand. We apply One Hot Encoding transformation on the `keyword` column so the model can also consider the prominent disaster related keywords as part of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72838b2-6e63-4d49-855c-836bfc8ba390",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08fde8b-6868-457b-934b-64531c111a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557bbe34-d961-4488-9231-6b7a23c37a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to report mean cross validation scores for different models\n",
    "\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08c183-f100-42d9-a29f-dd08ae4a30e0",
   "metadata": {},
   "source": [
    "#### Dummy Classifier (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f88130-1da6-4e83-a2b5-accd9f77e37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.189 (+/- 0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.190 (+/- 0.010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.190 (+/- 0.019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.188 (+/- 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.192 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.187 (+/- 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.501 (+/- 0.015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_roc_auc</th>\n",
       "      <td>0.496 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dummy\n",
       "fit_time         0.001 (+/- 0.001)\n",
       "score_time       0.004 (+/- 0.002)\n",
       "test_precision   0.189 (+/- 0.022)\n",
       "train_precision  0.190 (+/- 0.010)\n",
       "test_f1          0.190 (+/- 0.019)\n",
       "train_f1         0.188 (+/- 0.009)\n",
       "test_recall      0.192 (+/- 0.018)\n",
       "train_recall     0.187 (+/- 0.009)\n",
       "test_roc_auc     0.501 (+/- 0.015)\n",
       "train_roc_auc    0.496 (+/- 0.005)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc08d2-3984-46a2-8d1b-3174041a5f6d",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7c895-7404-488f-88d8-3fe1b16241b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=2000))\n",
    "results[\"logistic regression\"] = mean_std_cross_val_scores(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c826a-f66e-4f29-9cfc-53323ef238e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
